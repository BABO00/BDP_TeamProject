{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/leedohee/BDP_TeamProject/koBERT.ipynb 셀 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leedohee/BDP_TeamProject/koBERT.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#처음실행할 때 !pip install pyspark해주세요.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leedohee/BDP_TeamProject/koBERT.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#결과값으로 좋은단어10개, 안좋은단어10개, 정확도 나옵니다. \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leedohee/BDP_TeamProject/koBERT.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install pyspark\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leedohee/BDP_TeamProject/koBERT.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leedohee/BDP_TeamProject/koBERT.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m DoubleType\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leedohee/BDP_TeamProject/koBERT.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m udf, col, when\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#코랩 처음실행할 때 !pip install pyspark해주세요.\n",
    "#결과값으로 좋은단어10개, 안좋은단어10개, 정확도 나옵니다. \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Spark session creation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReviewAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a Spark DataFrame\n",
    "drive_path = \"/content/drive/MyDrive/kakaotalk_review.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(drive_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "df = df.withColumn(\"score\", df[\"score\"].cast(\"double\"))\n",
    "df = df.withColumn(\"thumbsUpCount\", df[\"thumbsUpCount\"].cast(\"double\"))\n",
    "df = df.dropna()\n",
    "\n",
    "# 'content' 열을 토큰화\n",
    "content_list = df.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
    "# 샘플링 및 배치 크기 조정\n",
    "content_list = content_list[:20]\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "tokenized = tokenizer(content_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# PyTorch DataLoader 생성\n",
    "dataset = TensorDataset(tokenized['input_ids'], tokenized['attention_mask'])\n",
    "# 샘플링 및 배치 크기 조정\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "# Model Inference using KoBERT\n",
    "model = BertForSequenceClassification.from_pretrained(\"monologg/kobert\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        print(\"Processing batch...\")\n",
    "        outputs = model(batch[0], attention_mask=batch[1])\n",
    "        predictions.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "\n",
    "# Convert predictions to Pandas DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"predicted_label\"])\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "predictions_col = spark.createDataFrame(predictions_df)\n",
    "\n",
    "# Add the predictions to the DataFrame\n",
    "df = df.join(predictions_col, how=\"outer\")\n",
    "\n",
    "# Add a new column 'predicted_label' with proper handling\n",
    "@udf(DoubleType())\n",
    "def extract_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "\n",
    "df = df.withColumn(\"predicted_label\", extract_float(col(\"predicted_label\")))\n",
    "\n",
    "# Add a new column 'sentiment' based on predicted labels\n",
    "df = df.withColumn(\"sentiment\", when(col(\"predicted_label\") == 1, \"Positive\").otherwise(\"Negative\"))\n",
    "\n",
    "# Function to get top N words and their counts from a list of tokenized words\n",
    "def get_top_words(words, n=10):\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_words[:n]\n",
    "\n",
    "# Get top positive and negative words with counts\n",
    "positive_reviews = df.filter(col(\"sentiment\") == \"Positive\").select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
    "negative_reviews = df.filter(col(\"sentiment\") == \"Negative\").select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "top_positive_words = get_top_words(positive_reviews)\n",
    "top_negative_words = get_top_words(negative_reviews)\n",
    "\n",
    "# Print top positive and negative words with counts\n",
    "print(\"Top Positive Words:\")\n",
    "for word, count in top_positive_words:\n",
    "    print(f\"{word}: {count} times\")\n",
    "\n",
    "print(\"Top Negative Words:\")\n",
    "for words, count in top_negative_words:\n",
    "    print(f\"{words}: {count} times\")\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"predicted_label\", labelCol=\"score\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(df)\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
